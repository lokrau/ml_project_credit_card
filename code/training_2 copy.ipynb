{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "\n",
    "from tensorflow.keras import layers, Model, optimizers, losses, callbacks\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.3, stratify=data[\"Is high risk\"], random_state=42)\n",
    "test, val = train_test_split(test, test_size=0.5, stratify=test[\"Is high risk\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(\"Is high risk\", axis=1)\n",
    "Y_train = train[\"Is high risk\"]\n",
    "\n",
    "X_test = test.drop(\"Is high risk\", axis=1)\n",
    "Y_test = test[\"Is high risk\"]\n",
    "\n",
    "X_val = val.drop(\"Is high risk\", axis=1)\n",
    "Y_val = val[\"Is high risk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  x = layers.Input(shape=(X_train.shape[1],))\n",
    "  h = layers.Dense(64, activation='relu')(x)\n",
    "  h = layers.Dense(64, activation='relu')(h)\n",
    "  h = layers.Dense(64, activation='relu')(h)\n",
    "  h = layers.Dense(64, activation='relu')(h)\n",
    "\n",
    "  y = layers.Dense(1, activation='sigmoid')(h)\n",
    "\n",
    "  model = Model(inputs=x, outputs=y)\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value \n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001])\n",
    "\n",
    "  optimizer = optimizers.Adam(learning_rate=hp_learning_rate, \n",
    "                            beta_1=0.9, \n",
    "                            beta_2=0.999, \n",
    "                            amsgrad=True)\n",
    "\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss=losses.BinaryCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete Folder optimization if it exists\n",
    "if os.path.exists('optimization'):\n",
    "    os.system('rm -rf optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=1000,\n",
    "                     factor=5,\n",
    "                     directory='optimization',\n",
    "                     project_name='ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    min_delta=0.001,     # Minimum change to qualify as an improvement\n",
    "    patience=10,         # How many epochs to wait after last time val loss improved\n",
    "    verbose=1,\n",
    "    mode='min',          # The training will stop when the quantity monitored has stopped decreasing\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 00m 03s]\n",
      "val_accuracy: 0.9829950928688049\n",
      "\n",
      "Best val_accuracy So Far: 0.9829950928688049\n",
      "Total elapsed time: 00h 00m 19s\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the optimal learning rate for the optimizer\n",
      "is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, Y_train, \n",
    "             epochs=1000, \n",
    "             validation_data=(X_val, Y_val), \n",
    "            #  callbacks=[early_stopping]\n",
    "             )\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# epochs = 1000\n",
    "\n",
    "# history = model.fit(X_train, \n",
    "#                     Y_train, \n",
    "#                     batch_size=batch_size,\n",
    "#                     validation_data=(X_val, Y_val),\n",
    "#                     callbacks=[early_stopping],\n",
    "#                     epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the model training history for accuracy\n",
    "# accuracy = history.history['accuracy']\n",
    "# val_accuracy = history.history['val_accuracy']\n",
    "# epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "# plt.plot(epochs, accuracy)\n",
    "# plt.plot(epochs, val_accuracy)\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoche')\n",
    "# plt.legend(['Training', 'Validation'], loc='lower left')\n",
    "# # plt.xticks(np.arange(1, len(accuracy)+1, 1))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print the model training history for loss\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# plt.plot(epochs, loss)\n",
    "# plt.plot(epochs, val_loss)\n",
    "# plt.title('Model Loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoche')\n",
    "# plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "# # plt.xticks(np.arange(1, len(loss)+1, 1))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_train, Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
